{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1axz_tPpF4ir7yZwnMlSk-DDoeB2x2139",
      "authorship_tag": "ABX9TyNFvbouTwtc6dDmXOWPQwqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegovianagomes/IMDB-Learning/blob/main/IMDB_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Initial Setup\n",
        "We´ll use the huggingface pre-trained models and we´ll also import all necessary  libraries into the pipeline."
      ],
      "metadata": {
        "id": "rXby7vZSSA6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdBycAGMRQmd"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "eKXHnuc7RgVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Control flags and Hyperparameters"
      ],
      "metadata": {
        "id": "t4jMty5ATcbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.2\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "-FGQxmWIS6RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU/GPU configuration\n"
      ],
      "metadata": {
        "id": "8TQdCUM8T2WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
        "print(\"Process unit check:\", device)"
      ],
      "metadata": {
        "id": "U9FDUhSIToI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and explore datasets\n",
        "\n",
        "In this section we'll download and read the IMDB dataset and explore the dataset."
      ],
      "metadata": {
        "id": "i0fmuO2NU_Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/imdb-reviews-pt-br.csv\")\n",
        "#df.head()"
      ],
      "metadata": {
        "id": "WrDDTrQ-gFhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset size"
      ],
      "metadata": {
        "id": "uKZ64aDqhL-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"number of examples: {len(df)}\")"
      ],
      "metadata": {
        "id": "BkzV-OuVgb6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of classes"
      ],
      "metadata": {
        "id": "7Ijs_SrhhcAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "FKWoiGHNhOX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YlQTO2cJhpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AHAlhp0uaqHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task demonstration in English\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gEIg-I1qh5US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline??"
      ],
      "metadata": {
        "id": "5oGVoXSai_jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_en = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "bert_en = pipeline('sentiment-analysis')"
      ],
      "metadata": {
        "id": "1tkj5xMXiA2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_instance = 100\n",
        "df['text_en'][test_instance], bert_en(df['text_en'][test_instance])"
      ],
      "metadata": {
        "id": "2jjPMqHsiN_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8xelpKccir5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task demonstration in Portugues\n",
        "### Tokenization\n",
        "---\n",
        "Our words must be inserted into the model as numbers, and in this case we'll use BERT tokeniser, which uses byte-pair encoding."
      ],
      "metadata": {
        "id": "fm9HJnACiwbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "metadata": {
        "id": "4bnf8v9piyCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is relatively small, we can store entirely in RAM, already tokenized.\n",
        "\n",
        "The cell below tokenizes all dataset\n",
        "---\n",
        "- df_tokenized will be a dictionary as keys ['input_ids', 'token_type_ids', 'attention_mask'\n",
        "- input_ids -> tokenized instances\n",
        "- token_type_ids -> mask used in classification tasks of pair phrases(It will be discarded in this task)\n",
        "- attention_mask -> mask of attention  that stands out for a model of tokens of padding [PAD]"
      ],
      "metadata": {
        "id": "u8plHcb-ltyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokenized = tokenizer.batch_encode_plus(df['text_pt'], return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "nJm86D63kZmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual inspection of the dataset format"
      ],
      "metadata": {
        "id": "pr_Pe8hCmlgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_tokenized[\"input_ids\"].shape, df_tokenized[\"attention_mask\"].shape)"
      ],
      "metadata": {
        "id": "FzEEeHeGmZ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting and defining X and Y, because the ideal is not to have a dictionary but a matrix, which can be seen with:\n",
        "\n",
        "- [0, DATASET_LEN, MAX_LENGTH] = input_ids\n",
        "- [1, DATASET_LEN, MAX_LENGTH] = attention_mask"
      ],
      "metadata": {
        "id": "vEp9f1nronb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.stack((df_tokenized['input_ids'], df_tokenized[\"attention_mask\"]), dim=0)\n",
        "\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 0 if x == 'neg' else 1)\n",
        "\n",
        "y = torch.Tensor(df['sentiment'].to_numpy())"
      ],
      "metadata": {
        "id": "2RaY-AHknz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "A dataloader is a set of Pytorch datasets, let's define a class that will be consumed by the DataLoader when we feed the model with data during training.\n",
        "\n",
        "To create a customised DataLoader in PyTorch, you need to follow a few important steps. Firstly, you must create a class that inherits from the Dataset base class, which is provided by PyTorch itself. This base class serves as a template to define how your data will be organised and accessed.\n",
        "\n",
        "Within this customised class, there are two essential methods that you need to implement:\n",
        "\n",
        "- __len__ : This method should return the total size of your dataset. It is used to tell the DataLoader how many samples are available in the dataset. For example, if your dataset contains 1000 images, the __len__ method should return the value 1000.\n",
        "- __getitem__ : This method is responsible for accessing a specific sample from the dataset. It takes an index as input (e.g. idx) and returns the sample corresponding to that index. This allows the DataLoader to know how to load each item individually."
      ],
      "metadata": {
        "id": "CVTrrIH7peTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.X = self.X.to(device)\n",
        "\n",
        "        self.y = y\n",
        "        self.y = self.y.to(device)\n",
        "\n",
        "        self.len = len(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[:, idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "VE9TuzhIooPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will be responsible for instantiating our training, validation and test loaders."
      ],
      "metadata": {
        "id": "KdXl2EY2qm5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(X, y)\n",
        "\n",
        "# Calculation of the number of instances that must exist in each split\n",
        "num_train_instances = int(np.round(dataset.len * TRAIN_RATIO))\n",
        "num_val_instances = int(np.round(dataset.len * VAL_RATIO))\n",
        "num_test_instances = int(np.round(dataset.len * TEST_RATIO))\n",
        "\n",
        "print(f\"TRAIN: {num_train_instances}, VAL: {num_val_instances}, TEST: {num_test_instances}\")\n",
        "\n",
        "\n",
        "train_split, val_split, test_split = torch.utils.data.random_split(dataset, [num_train_instances, num_val_instances, num_test_instances])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_split, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "deAQYDqnqx1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "ePhlJSMhtn8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "steps_per_epoch = 200\n",
        "epoch_validation_samples = 50\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased').to(device)\n",
        "\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False #True\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "acc_calc = lambda output, labels : (labels == output.argmax(axis=1)).sum()\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9997)"
      ],
      "metadata": {
        "id": "TNSg2hEYttg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_metadate = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    num_train_examples = 0\n",
        "    num_val_examples = 0\n",
        "\n",
        "    train_hits = 0\n",
        "    val_hits = 0\n",
        "\n",
        "    train_bar = tqdm(total = steps_per_epoch, desc = f\"Train\", unit = \"steps\", position = 0, leave = True)\n",
        "    val_bar = tqdm(total = steps_per_epoch, desc = f\"Val\", unit = \"samples\", position = 0, leave = True)\n",
        "\n",
        "    for batch_number, (features, labels) in enumerate(train_loader):\n",
        "        train_running_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        input_ids, input_masks, = features[:, 0, :], features[:, 1, :]\n",
        "\n",
        "        loss, logits = model(input_ids, input_masks, labels=labels.long())\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "        softmax_prediction = torch.nn.functional.softmax(logits, dim=1)\n",
        "        train_hits += acc_calc(softmax_prediction, labels)\n",
        "\n",
        "        train_bar.update(1)\n",
        "\n",
        "        num_train_examples += features.shape[0]\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if(batch_number + 1) % steps_per_epoch == 0:\n",
        "            train_bar.close()\n",
        "            break\n",
        "\n",
        "    for batch_number, (features, labels) in enumerate(val_loader):\n",
        "        val_running_loss = 0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        input_ids, input_masks, = features[:, 0, :], features[:, 1, :]\n",
        "\n",
        "        loss, logits = model(input_ids, input_masks, labels=labels.long())\n",
        "\n",
        "        val_running_loss += loss.item()\n",
        "        softmax_prediction = torch.nn.functional.softmax(logits, dim=1)\n",
        "        val_hits += acc_calc(softmax_prediction, labels)\n",
        "\n",
        "        num_val_examples += features.shape[0]\n",
        "\n",
        "        val_bar.update(1)\n",
        "\n",
        "        if(batch_number + 1) % steps_per_epoch == 0:\n",
        "            val_bar.close()\n",
        "            break\n",
        "\n",
        "train_acc = torch.true_divide(train_hits, num_train_examples)\n",
        "val_acc = torch.true_divide(val_hits, num_val_examples)\n",
        "\n",
        "print(F\"EPOCH SUMMARY - {I + 1} \\t Train Loss: {train_running_loss} \\t Train Acc: {train_acc} \\t Val Loss: {val_loss} \\t Val Acc: {val_acc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YkY3r8U0vaqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f'epoch_{i}')"
      ],
      "metadata": {
        "id": "5CFzOi8G27Lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}